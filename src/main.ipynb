{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Lens: Image Captioning with InceptionV3 & LSTM\n",
        "\n",
        "An end-to-end deep learning pipeline designed to bridge the gap between computer vision and natural language processing. This project automatically generates descriptive, human-like captions for any given image using the Flickr8k dataset.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Feature Extraction**: Utilizes InceptionV3 (pre-trained on ImageNet) to extract high-level visual features.\n",
        "\n",
        "- **Sequence Modeling**: Implements a Deep LSTM decoder to generate text sequences word-by-word.\n",
        "\n",
        "- **Search Strategies**: Features both Greedy Search and Beam Search (K-beams) for optimized sentence generation.\n",
        "\n",
        "- **Quantitative Evaluation**: Measures performance using BLEU-1 and BLEU-2 scores with smoothing functions.\n",
        "\n",
        "- **Custom Data Pipeline**: Efficiently handles large datasets using a Python generator and tf.data.Dataset to prevent memory overflow.\n",
        "\n",
        "## üõ†Ô∏è Tech Stack\n",
        "\n",
        "| Component               | Technology          |\n",
        "| ----------------------- | ------------------- |\n",
        "| Language                | Python 3            |\n",
        "| Deep Learning           | TensorFlow / Keras  |\n",
        "| Computer Vision         | InceptionV3         |\n",
        "| NLP NLTK (BLEU metrics) | Keras Tokenizer     |\n",
        "| Visualizations          | Matplotlib, Seaborn |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. System and Utility Libraries\n",
        "\n",
        "These modules handle file management, suppress noise in the output, and provide progress tracking.\n",
        "\n",
        "* **os & warnings**: Used for directory navigation and filtering out unnecessary deprecation warnings.\n",
        "* **re**: Regular expressions for text cleaning and preprocessing.\n",
        "* **tqdm**: Provides progress bars for long-running tasks like image feature extraction.\n",
        "* **Counter**: Used for vocabulary analysis and word frequency counts.\n",
        "\n",
        "## 2. Data Handling and Visualization\n",
        "\n",
        "Tools for managing numerical data and visualizing results.\n",
        "\n",
        "* **numpy & pandas**: The core stack for array manipulation and managing metadata (like image-caption mappings).\n",
        "* **matplotlib & seaborn**: Used to visualize training metrics and display images with their generated captions.\n",
        "* **PIL (Pillow)**: The standard library for loading and resizing raw image files.\n",
        "\n",
        "## 3. Deep Learning (TensorFlow/Keras)\n",
        "\n",
        "The architecture follows a **CNN-RNN encoder-decoder** pattern:\n",
        "\n",
        "### Image Processing (Encoder)\n",
        "\n",
        "* **InceptionV3**: A pre-trained model used for **Transfer Learning** to extract feature vectors from images.\n",
        "* **load_img & img_to_array**: Converts raw images into the format required by the neural network.\n",
        "\n",
        "### Text Processing & Modeling (Decoder)\n",
        "\n",
        "* **Tokenizer & pad_sequences**: Converts words into numeric sequences and ensures uniform input lengths.\n",
        "* **LSTM & Embedding**: Layers designed to handle sequential data and capture the semantic relationship between words.\n",
        "* **Dense, Dropout, BatchNormalization**: Layers used to regularize the model and map features to the final vocabulary size.\n",
        "\n",
        "## 4. Evaluation Metrics\n",
        "\n",
        "* **BLEU Score (NLTK)**: The primary metric used to evaluate the quality of the generated captions by comparing them against human-written references.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MinjrUtKzolP"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import regex as re\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, add\n",
        "from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Acquisition: Flickr8K\n",
        "\n",
        "This cell automates the retrieval of the **Flickr8K dataset** directly from Kaggle. This dataset is a benchmark in the field of image captioning, consisting of 8,000 images each paired with five different human-annotated descriptions.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Automated Downloading\n",
        "\n",
        "The code utilizes the kagglehub library to fetch the data programmatically, ensuring the environment is reproducible without manual file uploads.\n",
        "\n",
        "* **kagglehub.dataset_download**: Downloads the specific adityajn105/flickr8k repository. This includes:\n",
        "* **Images**: A folder containing the raw .jpg files.\n",
        "* **Captions**: A text file (usually captions.txt) mapping image filenames to their respective descriptions.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Directory Verification\n",
        "\n",
        "After the download completes, the script performs a health check on the file path:\n",
        "\n",
        "* **path_to_dataset**: Stores the absolute local path where the files were saved.\n",
        "* **os.listdir**: Iterates through the downloaded directory to confirm the presence of key files (e.g., the Images folder and the metadata files).\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Directory Structure\n",
        "\n",
        "Once successfully executed, you should see a file structure similar to this:\n",
        "\n",
        "* Images/: 8,091 images in JPEG format.\n",
        "* captions.txt: A CSV/text file containing the image IDs and their corresponding captions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7eHdWgLwkYw",
        "outputId": "421eaa3c-cf9e-4fda-955f-03288e05c6eb"
      },
      "outputs": [],
      "source": [
        "# Import the flickr8k dataset from kaggle\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# 1. Download the latest version\n",
        "path_to_dataset = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "\n",
        "print(f\"Path to dataset files: {path_to_dataset}\")\n",
        "\n",
        "# 2. List the contents of the directory\n",
        "files = os.listdir(path_to_dataset)\n",
        "\n",
        "print(\"\\nContents of the directory:\")\n",
        "for file in files:\n",
        "    print(f\"- {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loading and Text Tokenization\n",
        "\n",
        "This cell defines the initial functions for ingestion and text processing. It transforms the raw text file from the Flickr8K dataset into a format suitable for natural language modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Path Configuration\n",
        "\n",
        "* **images_directory**: Defines the source folder containing the .jpg files.\n",
        "* **captions_path**: Targets the specific text file where image IDs are mapped to their descriptions.\n",
        "\n",
        "## 2. Caption Ingestion (load_captions)\n",
        "\n",
        "This function reads the metadata and performs basic text normalization:\n",
        "\n",
        "* **Header Removal**: It slices the list [1:] to skip the CSV header (e.g., \"image,caption\").\n",
        "* **Case Normalization**: Converts all text to **lowercase** using a list comprehension. This reduces the vocabulary size by ensuring \"Dog\" and \"dog\" are treated as the same token.\n",
        "\n",
        "## 3. Text Vectorization (tokenize_captions)\n",
        "\n",
        "This function prepares the text for the neural network using the Keras Tokenizer:\n",
        "\n",
        "* **Vocabulary Building**: fit_on_texts analyzes the entire caption set to create an internal dictionary of every unique word.\n",
        "* **Word-to-Index Mapping**: This step is crucial for converting human-readable sentences into sequences of integers that the **Embedding layer** of the model can process.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Execution and Validation\n",
        "\n",
        "* The script calls load_captions to create the initial list of strings.\n",
        "* **captions[:15:3]**: This slice prints every 3rd caption from the first 15 entries. This is a quick diagnostic check to verify that the text has been loaded correctly and the lowercase normalization was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7ku7RQe0auA",
        "outputId": "39d86275-bb34-452d-ecae-8b77b0ffdf5e"
      },
      "outputs": [],
      "source": [
        "images_directory = '/kaggle/input/flickr8k/Images/'\n",
        "captions_path = '/kaggle/input/flickr8k/captions.txt'\n",
        "def load_captions(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        captions = f.readlines()\n",
        "        captions = [caption.lower() for caption in captions[1:]]\n",
        "    return captions\n",
        "\n",
        "def tokenize_captions(captions):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    return tokenizer\n",
        "\n",
        "captions = load_captions(captions_path)\n",
        "captions[:15:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Preprocessing and Cleaning\n",
        "\n",
        "This cell implements a text cleaning pipeline to remove noise from the captions. Refining the text data is a critical step in reducing the model's vocabulary complexity and improving the accuracy of the generated descriptions.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The clean_text Function\n",
        "\n",
        "This function applies a series of **Regular Expressions (Regex)** to standardize each caption:\n",
        "\n",
        "- **Punctuation Removal**: re.sub(r'[^\\w\\s]', '', text) strips out all special characters and punctuation (e.g., commas, periods, exclamation marks).\n",
        "- **Digit Removal**: re.sub(r'\\d+', '', text) removes numbers, as they are often irrelevant to the general visual description of these images.\n",
        "- **Whitespace Normalization**: re.sub(r'\\s+', ' ', text).strip() replaces multiple spaces or tabs with a single space and trims leading/trailing whitespace.\n",
        "\n",
        "## 2. Data Transformation\n",
        "\n",
        "The cleaning process is applied to the raw dataset using a list comprehension:\n",
        "\n",
        "- **Field Extraction**: caption.split(',')[1] separates the Image ID from the actual caption text (assuming a CSV-style format).\n",
        "- **Batch Processing**: The clean_text function is called for every caption in the list, resulting in the cleaned_captions array.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Visualizing the Transformation\n",
        "\n",
        "The cleaning process reduces \"dirty\" text into a \"clean\" format that the model can more easily learn.\n",
        "\n",
        "| Input (Raw)                                        | Output (Cleaned)        |\n",
        "| -------------------------------------------------- | ----------------------- |\n",
        "| 1000268201_693b08cb0e.jpg,A child in a pink dress. | a child in a pink dress |\n",
        "| 12345.jpg, Two dogs running!!                      | two dogs running        |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Verification\n",
        "\n",
        "- **cleaned_captions[:15:2]**: This command prints every second cleaned caption from the first 15 entries. This allows for a manual inspection to ensure that punctuation and numbers have been successfully removed before the text is passed to the tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBbtegqD0aqB",
        "outputId": "87c494a1-a23c-439e-ffb5-ccb52ff75af4"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "cleaned_captions = [clean_text(caption.split(',')[1]) for caption in captions]\n",
        "cleaned_captions[:15:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sequence Tagging and ID Mapping\n",
        "\n",
        "This cell prepares the final text format required for the **Encoder-Decoder** model architecture. It links the cleaned captions back to their respective image identifiers and adds structural markers for the sequence generation process.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Sequence Token Injection\n",
        "\n",
        "The code adds two special \"pseudo-words\" to every caption:\n",
        "\n",
        "- **start**: A \"Start-of-Sequence\" (SOS) token that signals to the model where to begin generating a description.\n",
        "- **end**: An \"End-of-Sequence\" (EOS) token that tells the model when to stop generating words.\n",
        "\n",
        "These tokens are essential for the inference phase, where the model must decide when a sentence is grammatically complete.\n",
        "\n",
        "## 2. Image-to-Caption Mapping\n",
        "\n",
        "The loop re-associates the text with its source image:\n",
        "\n",
        "- **split(',')[0]**: Extracts the unique Image ID (e.g., 1000268201_693b08cb0e.jpg) from the original raw data.\n",
        "- **Tab-Separated Formatting**: The ID and the tagged caption are joined using a \\t (tab) delimiter.\n",
        "- **Newline Addition**: A \\n is added to ensure each entry is treated as a distinct line for future file exports or batch processing.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Data Structure Evolution\n",
        "\n",
        "| Stage             | Data Format Example                |\n",
        "| ----------------- | ---------------------------------- |\n",
        "| **Raw**           | 123.jpg,A dog runs.                |\n",
        "| **Cleaned**       | a dog runs                         |\n",
        "| **Final Mapping** | 123.jpg \\t start a dog runs end \\n |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Verification and Dataset Scale\n",
        "\n",
        "- **captions_IDs[:20:3]**: Samples every third entry from the first twenty to confirm the start, end, and tab-spacing are correctly applied.\n",
        "- **len(captions_IDs)**: Returns the total count of captions in the dataset (expected to be approximately **40,455** for Flickr8K, representing 5 captions for each of the 8,091 images).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1CUb34R0kRJ",
        "outputId": "4b5d99c6-cb04-4be0-d9c1-b959a09b007e"
      },
      "outputs": [],
      "source": [
        "captions_IDs = []\n",
        "for i in range(len(cleaned_captions)):\n",
        "    item = captions[i].split(',')[0]+'\\t'+'start '+cleaned_captions[i]+' end\\n'\n",
        "    captions_IDs.append(item)\n",
        "\n",
        "captions_IDs[:20:3], len(captions_IDs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Visualization: Images and Captions\n",
        "\n",
        "This cell defines a function to visually inspect the dataset. It creates a side-by-side display of images and their corresponding multi-sentence descriptions to verify that the text cleaning and image mapping steps were successful.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Dictionary Construction\n",
        "\n",
        "Because each image has five unique captions, the function first reorganizes the flat captions_IDs list into a more manageable structure:\n",
        "\n",
        "* **captions_dictionary**: Groups all descriptions under a single image filename key.\n",
        "* **Slicing**: The function specifically samples from index 100 to show a variety of images later in the dataset.\n",
        "\n",
        "## 2. Image Loading and Subplot Layout\n",
        "\n",
        "The function uses matplotlib to create a grid for visual comparison:\n",
        "\n",
        "* **load_img**: Loads the raw image from the directory and resizes it to  pixels for consistent display.\n",
        "* **Grid Logic**: It creates a subplot with num_of_images rows and 2 columns:\n",
        "* **Column 1**: Displays the actual image.\n",
        "* **Column 2**: An empty plot area used purely for rendering the text of the five captions.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Text Rendering\n",
        "\n",
        "Instead of printing text to the console, the function uses the plot's coordinate system to draw the captions:\n",
        "\n",
        "* **ax.text()**: Places each of the 5 captions at specific vertical intervals.\n",
        "* **Formatting**: Sets the font size to 20 and hides the plot axes (axis('off')) to ensure the descriptions are legible and the UI is clean.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Execution Logic\n",
        "\n",
        "* **visualaization(captions_IDs, 5)**: This call generates a large figure showing 5 distinct images and their 25 total captions (5 per image). This is a vital qualitative check to ensure the \"start\" and \"end\" tokens are correctly placed and the image-caption alignment is accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IwThvey60kbJ",
        "outputId": "0582e8f3-7bf2-463c-a048-efcbcce9a74e"
      },
      "outputs": [],
      "source": [
        "def visualaization(data, num_of_images):\n",
        "    captions_dictionary = {}\n",
        "    for item in data[100:100+(num_of_images)*5]:\n",
        "        image_id, caption = item.split('\\t')\n",
        "        if image_id not in captions_dictionary:\n",
        "            captions_dictionary[image_id] = []\n",
        "        captions_dictionary[image_id].append(caption)\n",
        "    else:\n",
        "        list_captions = [x for x in captions_dictionary.items()]\n",
        "\n",
        "    count = 1\n",
        "    fig = plt.figure(figsize=(10,20))\n",
        "    for filename in list(captions_dictionary.keys()):\n",
        "        captions = captions_dictionary[filename]\n",
        "        image_load = load_img(images_directory+filename, target_size=(199,199,3))\n",
        "\n",
        "        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n",
        "        ax.imshow(image_load)\n",
        "        count += 1\n",
        "\n",
        "        ax = fig.add_subplot(num_of_images,2,count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,len(captions))\n",
        "        for i, caption in enumerate(captions):\n",
        "            ax.text(0,i,caption,fontsize=20)\n",
        "        count += 1\n",
        "    plt.show()\n",
        "\n",
        "visualaization(captions_IDs, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis: Caption Length Distribution\n",
        "\n",
        "This cell performs a statistical analysis of the caption lengths. Understanding the distribution of word counts is a prerequisite for determining the optimal sequence length for the neural network's input layers.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Sequence Length Analysis\n",
        "\n",
        "The function captions_length processes the cleaned_captions list to visualize the frequency of different sentence lengths:\n",
        "\n",
        "* **Word Counting**: It calculates the length of each caption using a list comprehension: [len(x.split(' ')) for x in data].\n",
        "* **Histogram and KDE**:\n",
        "* **Histogram**: Displays the count (frequency) of captions for each specific word count.\n",
        "* **KDE (Kernel Density Estimate)**: Overlays a smooth curve to show the underlying probability density, helping to identify the \"typical\" caption length.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Importance for Model Configuration\n",
        "\n",
        "This visualization is critical for setting the **max_length** parameter in the preprocessing pipeline:\n",
        "\n",
        "* **Padding Strategy**: Most deep learning models require fixed-size input vectors. By looking at this plot, we can choose a maximum length that covers the vast majority of captions without being excessively long (which would lead to sparse data and wasted computation).\n",
        "* **Outlier Detection**: Identifies if there are unusually long captions that might need to be truncated or removed.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Visualization Parameters\n",
        "\n",
        "* **binwidth=1**: Ensures that each bar in the histogram represents exactly one word increment.\n",
        "* **Styling**: Uses darkgrid and high-resolution settings (dpi=300) to ensure the plot is clear and professional.\n",
        "* **Axes**:\n",
        "* **X-axis (Length)**: Represents the total number of words in a caption.\n",
        "* **Y-axis (Frequency)**: Represents how many captions in the dataset have that specific length.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Expected Insights\n",
        "\n",
        "Typically, for the Flickr8K dataset, the distribution is skewed:\n",
        "\n",
        "* Most captions are between  and  words long.\n",
        "* Very few captions exceed  or  words.\n",
        "* Based on this plot, a max_length of roughly  to  is usually selected to ensure no information is lost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "3f7602Fx0kdk",
        "outputId": "3a5ba08d-1774-4ac4-cc56-e58421f3ce55"
      },
      "outputs": [],
      "source": [
        "def captions_length(data):\n",
        "    plt.figure(figsize=(15, 7), dpi=300)\n",
        "    sns.set_style('darkgrid')\n",
        "    sns.histplot(x=[len(x.split(' ')) for x in data], kde=True, binwidth=1)\n",
        "    plt.title('Captions length histogram', fontsize=15, fontweight='bold')\n",
        "    plt.xticks(fontweight='bold')\n",
        "    plt.yticks(fontweight='bold')\n",
        "    plt.xlabel('Length', fontweight='bold')\n",
        "    plt.ylabel('Freaquency', fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "captions_length(cleaned_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vocabulary Initialization and Tokenization\n",
        "\n",
        "This cell executes the tokenization process for the entire corpus and calculates the final vocabulary size. This is a foundational step for the **Embedding layer** of the neural network.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Tokenizer Execution\n",
        "\n",
        "The code calls the previously defined tokenize_captions function on the cleaned_captions list.\n",
        "\n",
        "* **Internal Mapping**: The Tokenizer scans every word in the dataset and creates a unique integer index for each one (e.g., \"dog\"  1, \"cat\"  2).\n",
        "* **Word Indexing**: It builds a dictionary accessible via tokenizer.word_index, which maps every unique word to its corresponding numeric ID.\n",
        "\n",
        "## 2. Vocabulary Size Calculation\n",
        "\n",
        "The variable vocab_size represents the total number of unique words the model will be able to recognize and generate.\n",
        "\n",
        "* **len(tokenizer.word_index)**: Counts the unique words found in the cleaned captions.\n",
        "* **The + 1 Addition**: In Keras, index 0 is reserved for **padding** (used to make sequences the same length). Therefore, the total vocabulary size must be the number of words plus one to account for this reserved index.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Structural Significance\n",
        "\n",
        "The vocab_size is a critical hyperparameter for the model architecture:\n",
        "\n",
        "* **Embedding Layer**: Defines the input dimension of the embedding table.\n",
        "* **Output Layer**: Determines the number of neurons in the final Dense layer (using Softmax) to predict the next word in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Output\n",
        "\n",
        "* **vocab_size**: Printing this value reveals the total complexity of the language the model needs to learn. For the Flickr8K dataset, this is typically around **8,000 to 9,000** unique tokens depending on the cleaning steps used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO2C5ttB0kfu",
        "outputId": "69b7a654-4eca-498a-ffe0-32f253597a48"
      },
      "outputs": [],
      "source": [
        "tokenizer = tokenize_captions(cleaned_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Splitting and Distribution\n",
        "\n",
        "This cell implements the data partitioning strategy, ensuring that the model is trained, validated, and tested on mutually exclusive sets of images. This prevents data leakage and allows for an unbiased evaluation of the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Directory Listing\n",
        "\n",
        "* **all_image_ids**: Retrieves the filenames for all 8,091 images in the Flickr8K dataset. This list serves as the primary index for the splitting logic.\n",
        "\n",
        "## 2. Three-Way Data Split\n",
        "\n",
        "The code utilizes train_test_split twice to divide the image IDs into three distinct categories:\n",
        "\n",
        "* **Train ()**: Used to update model weights and learn the relationship between images and text.\n",
        "* **Validation ()**: Used during the training process to monitor for overfitting and tune hyperparameters.\n",
        "* **Test ()**: Reserved for the final evaluation to see how the model generalizes to completely unseen images.\n",
        "\n",
        "## 3. Caption Categorization\n",
        "\n",
        "Since each image is associated with 5 captions, the code iterates through captions_IDs to ensure that all 5 captions for a specific image stay within the same split:\n",
        "\n",
        "* **Lookup Logic**: It extracts the image_id from the tab-separated string and checks which ID set (Train, Val, or Test) it belongs to.\n",
        "* **Consistency**: This approach ensures that if an image is in the test set, the model has never seen *any* of its descriptions during training.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Summary and Verification\n",
        "\n",
        "The final line provides a diagnostic summary of the split:\n",
        "\n",
        "* **Sample View**: Displays the first caption from each split to verify the format.\n",
        "* **Image Counts**: The len(...) / 5 calculation converts the number of individual captions back into the number of unique images per set.\n",
        "\n",
        "| Split | Percentage | Purpose |\n",
        "| --- | --- | --- |\n",
        "| **Train** |  | Model learning and weight optimization. |\n",
        "| **Validation** |  | Tuning and early stopping. |\n",
        "| **Test** |  | Final performance benchmarking. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ArJLc3s0kh4",
        "outputId": "c1056112-056f-47d1-92dc-7cdaaaeb81f7"
      },
      "outputs": [],
      "source": [
        "all_image_ids = os.listdir(images_directory)\n",
        "\n",
        "train_image_ids, val_image_ids = train_test_split(all_image_ids, test_size=0.15, random_state=42)\n",
        "val_image_ids, test_image_ids = train_test_split(val_image_ids, test_size=0.1, random_state=42)\n",
        "\n",
        "train_captions, val_captions, test_captions = [], [], []\n",
        "for caption in captions_IDs:\n",
        "    image_id, _ = caption.split('\\t')\n",
        "\n",
        "    if image_id in train_image_ids:\n",
        "        train_captions.append(caption)\n",
        "\n",
        "    elif image_id in val_image_ids:\n",
        "        val_captions.append(caption)\n",
        "\n",
        "    elif image_id in test_image_ids:\n",
        "        test_captions.append(caption)\n",
        "\n",
        "    else:\n",
        "        print('Unknown image ID !')\n",
        "\n",
        "train_captions[0], val_captions[0], test_captions[0], len(train_captions)/5, len(val_captions)/5, len(test_captions)/5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Preprocessing and Feature Extraction Model\n",
        "\n",
        "This cell prepares the **Encoder** portion of the Image Captioning system. It uses **Transfer Learning** to leverage a powerful pre-trained Convolutional Neural Network (CNN) for identifying visual patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Image Preprocessing Pipeline (preprocess_image)\n",
        "\n",
        "Before an image can be processed by a CNN, it must be transformed into a specific mathematical format:\n",
        "\n",
        "* **Resizing**: load_img(..., target_size=(299, 299)) scales images to the exact dimensions required by the InceptionV3 architecture.\n",
        "* **Array Conversion**: img_to_array converts the visual image into a 3D NumPy array of pixel values.\n",
        "* **Dimensional Expansion**: expand_dims(..., axis=0) adds a \"batch dimension,\" turning the  array into a  tensor.\n",
        "* **Normalization**: preprocess_input scales pixel values (typically between -1 and 1) to match the distribution of the original ImageNet training data.\n",
        "\n",
        "## 2. Feature Extraction (extract_image_features)\n",
        "\n",
        "This function passes the preprocessed image through the model to obtain a \"bottleneck\" feature vector.\n",
        "\n",
        "* **model.predict**: Runs the forward pass of the neural network.\n",
        "* **verbose=0**: Suppresses the prediction progress bar for a cleaner output during batch processing.\n",
        "\n",
        "## 3. InceptionV3 Model Modification\n",
        "\n",
        "The script downloads the **InceptionV3** model but modifies its architecture for feature extraction rather than classification:\n",
        "\n",
        "* **Weights ('imagenet')**: Loads the model pre-trained on millions of images from the ImageNet database.\n",
        "* **Removing the Head**: The final classification layer (which predicts object categories like \"dog\" or \"car\") is removed using pop().\n",
        "* **The Functional API**: Model(inputs=..., outputs=...) creates a new model that outputs the contents of the **Global Average Pooling** layer (or the second-to-last layer).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Architectural Goal\n",
        "\n",
        "By truncating the model, we treat the CNN as a \"vision-to-math\" converter. Instead of getting a class label, we receive a high-dimensional vector (usually of size **2048**) that represents the semantic content of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZd4ZdmT0kj7",
        "outputId": "42d0a74e-20d6-4da9-bb8a-f3ac8eed5009"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(299, 299))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def extract_image_features(model, image_path):\n",
        "    img = preprocess_image(image_path)\n",
        "    features = model.predict(img, verbose=0)\n",
        "    return features\n",
        "\n",
        "inception_v3_model = InceptionV3(weights = 'imagenet', input_shape=(299, 299, 3))\n",
        "inception_v3_model.layers.pop()\n",
        "inception_v3_model = Model(inputs=inception_v3_model.inputs, outputs=inception_v3_model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Feature Extraction and Persistence\n",
        "\n",
        "This cell performs the computationally intensive task of passing every image in the dataset through the modified InceptionV3 model to extract their visual signatures.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Feature Storage Structure\n",
        "\n",
        "The code initializes three dictionaries: train_image_features, val_image_features, and test_image_features.\n",
        "\n",
        "* **Key**: The unique Image ID (filename).\n",
        "* **Value**: A high-dimensional numerical vector representing the image's content.\n",
        "* **Flattening**: image_features.flatten() converts the multidimensional output from the CNN into a 1D array (typically of size **2048**), making it compatible with the subsequent dense layers of the captioning model.\n",
        "\n",
        "## 2. Progress Tracking (tqdm)\n",
        "\n",
        "Because processing over 8,000 images through a deep neural network is time-consuming, a progress bar is implemented:\n",
        "\n",
        "* **total=len(all_image_ids)**: Sets the bar's length to the total number of images.\n",
        "* **colour='green'**: Provides a visual cue in the notebook environment to monitor the extraction status in real-time.\n",
        "\n",
        "## 3. Extraction Logic\n",
        "\n",
        "The loop iterates through every image ID and performs the following:\n",
        "\n",
        "1. **Path Construction**: Combines the directory path with the image ID to locate the file.\n",
        "2. **Inference**: Calls extract_image_features to perform a forward pass through InceptionV3.\n",
        "3. **Conditional Sorting**: Assigns the resulting vector to the appropriate dictionary based on whether the image belongs to the **Train**, **Validation**, or **Test** split.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Technical Impact\n",
        "\n",
        "By extracting and storing these features now, we avoid running the heavy InceptionV3 model during every epoch of the training phase. This **Feature Caching** strategy:\n",
        "\n",
        "* **Saves Time**: Training will be significantly faster since the model only needs to learn the mapping from a static vector to a sequence of words.\n",
        "* **Saves Memory**: We no longer need to keep the large InceptionV3 model in GPU memory once this loop completes.\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Data Shape\n",
        "\n",
        "| Component | Representation | Size (Typical) |\n",
        "| --- | --- | --- |\n",
        "| **Input** | Raw Image () | 268,203 values |\n",
        "| **Output** | Flattened Feature Vector | 2,048 values |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "33f2e715f0294c1c9cefcabffaf05545",
            "866e28c31ed741619e5b4c0274241c55",
            "51aea74cf7c84971912bac863971a969",
            "9973401db03a44d285dc1ae5b69f5196",
            "9cc5e9fb565743f49eb9f8425da3fdf1",
            "d819267408164696b9ccd54b139a920d",
            "09d9b5b452bc43cb8f66669966e8d19b",
            "8f5fc864c383485dbb2acb6b191a646c",
            "cff09414f7714ab3b66d356935467ccf",
            "a125a3403d1b4c28a7f3498942248e10",
            "68f9dc1e5a704ce3b0011a723e6e0533"
          ]
        },
        "id": "3wSwgVYS0kmB",
        "outputId": "f07580d1-d10c-48f8-a626-16090132d760"
      },
      "outputs": [],
      "source": [
        "train_image_features, val_image_features, test_image_features = {}, {}, {}  # A Dictionary to store image features with their corresponding IDs\n",
        "\n",
        "pbar = tqdm_notebook(total=len(all_image_ids), position=0, leave=True, colour='green')\n",
        "\n",
        "for caption in all_image_ids:\n",
        "    image_id = caption.split('\\t')[0]\n",
        "    image_path = os.path.join(images_directory, image_id)\n",
        "    image_features = extract_image_features(inception_v3_model, image_path) # Extracting features\n",
        "\n",
        "    if image_id in train_image_ids:\n",
        "        train_image_features[image_id] = image_features.flatten()  # Flattening the features\n",
        "        pbar.update(1)\n",
        "\n",
        "    elif image_id in val_image_ids:\n",
        "        val_image_features[image_id] = image_features.flatten()  # Flattening the features\n",
        "        pbar.update(1)\n",
        "\n",
        "    elif image_id in test_image_ids:\n",
        "        test_image_features[image_id] = image_features.flatten()  # Flattening the features\n",
        "        pbar.update(1)\n",
        "\n",
        "    else:\n",
        "        print('Unknown image ID !')\n",
        "\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Learning Data Generator\n",
        "\n",
        "This cell implements the **Data Generator** pattern, which is essential for training deep learning models on large datasets. Instead of loading the entire processed dataset into RAM (which would cause a memory overflow), this generator creates training samples **on-the-fly**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Generator Logic (data_generator)\n",
        "\n",
        "The function follows a \"Progressive Step\" approach to teach the model how to predict the next word in a sequence:\n",
        "\n",
        "* **Shuffling**: np.random.shuffle(image_ids) ensures the model doesn't learn the order of the images, improving generalization.\n",
        "* **Sequence Slicing**: For a caption like start a dog runs end, the generator creates multiple training pairs:\n",
        "* **Input:** start  **Target:** a\n",
        "* **Input:** start a  **Target:** dog\n",
        "* **Input:** start a dog  **Target:** runs\n",
        "\n",
        "\n",
        "* **Padding**: pad_sequences ensures all input text vectors have a uniform length (max_caption_length), filling empty slots with zeros.\n",
        "* **One-Hot Encoding**: to_categorical converts the target word index into a sparse vector of the size of the vocabulary.\n",
        "\n",
        "## 2. Parameter Configuration\n",
        "\n",
        "* **max_caption_length**: Dynamically calculated based on the longest caption in the dataset. Adding + 1 ensures there is room for the \"start/end\" tokens.\n",
        "* **cnn_output_dim**: Captured as **2048**, matching the output of the InceptionV3 bottleneck layer.\n",
        "* **batch_size**: Set to **270** for training and **150** for validation. This balances computational speed with memory stability.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Data Flow Summary\n",
        "\n",
        "The generator yields a dual-input structure required by the **Merge Architecture** model:\n",
        "\n",
        "| Input Component | Description | Shape |\n",
        "| --- | --- | --- |\n",
        "| **X_images** | Pre-extracted CNN features. | (Batch, 2048) |\n",
        "| **X_captions** | Padded integer sequences of words. | (Batch, max_length) |\n",
        "| **y (Target)** | One-hot encoded \"next word\" in sequence. | (Batch, vocab_size) |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Operational Setup\n",
        "\n",
        "* **train_data_generator**: Supplies the training loop with infinite batches of randomized image-text pairs.\n",
        "* **val_data_generator**: Supplies the validation loop to monitor performance on unseen data during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u1v2enq30koR"
      },
      "outputs": [],
      "source": [
        "def data_generator(captions, image_features, tokenizer, max_caption_length, batch_size):\n",
        "    num_samples = len(captions)\n",
        "    image_ids = list(image_features.keys())\n",
        "    while True:\n",
        "        np.random.shuffle(image_ids)  # Shuffle image_ids for each epoch\n",
        "        for start_idx in range(0, num_samples, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, num_samples)\n",
        "            X_images, X_captions, y = [], [], []\n",
        "            for caption in captions[start_idx:end_idx]:\n",
        "                image_id, caption_text = caption.split('\\t')\n",
        "                caption_text = caption_text.rstrip('\\n')\n",
        "                seq = tokenizer.texts_to_sequences([caption_text])[0] # Tokenizing the caption\n",
        "                for i in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i] # X_caption, Y\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    X_images.append(image_features[image_id])\n",
        "                    X_captions.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            yield [np.array(X_images), np.array(X_captions)], np.array(y)\n",
        "\n",
        "\n",
        "max_caption_length = max(len(caption.split()) for caption in cleaned_captions) + 1\n",
        "\n",
        "cnn_output_dim = inception_v3_model.output_shape[1] # 2048\n",
        "\n",
        "batch_size_train = 270\n",
        "batch_size_val = 150\n",
        "\n",
        "train_data_generator = data_generator(train_captions, train_image_features, tokenizer, max_caption_length, batch_size_train)\n",
        "val_data_generator = data_generator(val_captions, val_image_features, tokenizer, max_caption_length, batch_size_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Architecture: The Merge Architecture\n",
        "\n",
        "This cell defines and compiles the core neural network. The design follows the **Merge Architecture**, which treats the image features and the text sequences as two separate encoded inputs that are combined later in the pipeline to predict the next word.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Encoder Sub-networks\n",
        "\n",
        "The model processes two distinct types of data simultaneously:\n",
        "\n",
        "### Image Feature Branch (Input 1)\n",
        "\n",
        "* **Input_image**: Receives the 2048-dimensional vector from the InceptionV3 model.\n",
        "* **BatchNormalization**: Standardizes the features to speed up training and provide regularization.\n",
        "* **Dense(256)**: Projects the high-dimensional image features into a 256-dimensional space to match the size of the language model's output.\n",
        "\n",
        "### Sequence Processing Branch (Input 2)\n",
        "\n",
        "* **Input_caption**: Receives the padded integer sequences of captions.\n",
        "* **Embedding**: Converts integer word IDs into dense vectors of size 256. The mask_zero=True parameter tells the model to ignore the padding zeros.\n",
        "* **LSTM(256)**: A Long Short-Term Memory layer that captures the temporal dependencies and context of the words in the caption.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Decoder (The Merge Layer)\n",
        "\n",
        "* **add([fe3, se2])**: This is the \"Merge\" step. It element-wise adds the processed image vector and the LSTM output, effectively injecting the visual context into the linguistic sequence.\n",
        "* **Dense(256)**: A final hidden layer to learn the non-linear relationship between the merged features.\n",
        "* **Output_Layer**: A Dense layer with a **Softmax** activation and a size equal to vocab_size. It outputs a probability distribution across all possible words in the vocabulary for the \"next\" word in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training Configuration\n",
        "\n",
        "* **Loss Function**: categorical_crossentropy is used because the task is a multi-class classification problem (predicting one word out of thousands).\n",
        "* **Optimizer**: **Adam** with a learning rate of 0.01.\n",
        "* **clipnorm=1.0**: Implements gradient clipping to prevent \"exploding gradients,\" a common issue in LSTM training where gradients become too large and cause instability.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model Summary\n",
        "\n",
        "* **caption_model.summary()**: Displays the layer-by-layer breakdown, showing the number of trainable parameters. This is useful for verifying that the shapes of the image and text branches align correctly before starting the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HDObXBwy0kqe",
        "outputId": "c09f65ad-783e-4096-84a7-81e326062055"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, max_caption_length, cnn_output_dim):\n",
        "    input_image = Input(shape=(cnn_output_dim,), name='Features_Input')\n",
        "    fe1 = BatchNormalization()(input_image)\n",
        "    fe2 = Dense(256, activation='relu')(fe1) # Adding a Dense layer to the CNN output to match the decoder output size\n",
        "    fe3 = BatchNormalization()(fe2)\n",
        "\n",
        "    input_caption = Input(shape=(max_caption_length,), name='Sequence_Input')\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(input_caption)\n",
        "    se2 = LSTM(256)(se1)\n",
        "\n",
        "    decoder1 = add([fe3, se2])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax', name='Output_Layer')(decoder2)\n",
        "\n",
        "    model = Model(inputs=[input_image, input_caption], outputs=outputs, name='Image_Captioning')\n",
        "    return model\n",
        "\n",
        "caption_model = build_model(vocab_size, max_caption_length, cnn_output_dim)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01, clipnorm=1.0)\n",
        "caption_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "caption_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Visualization and Documentation\n",
        "\n",
        "This cell generates a graphical representation of the neural network's architecture, providing a visual map of how data flows from the image and text inputs to the final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "The plot_model function creates a flowchart of the model's layers. This is essential for verifying the **Dual-Input Merge Architecture**:\n",
        "\n",
        "* **Parallel Inputs**: You can see the Features_Input (Image) and Sequence_Input (Text) branches starting independently.\n",
        "* **Feature Processing**: Shows the sequence of Batch Normalization, Dense, and LSTM layers.\n",
        "* **The Merge Point**: Highlights the add layer where visual and textual information are fused.\n",
        "* **Prediction Head**: Shows the final transition to the vocab_size output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LIT_tEdX0ksz",
        "outputId": "d90ff999-3477-436b-881e-c947cfa1df8a"
      },
      "outputs": [],
      "source": [
        "plot_model(caption_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Data Pipeline and Model Training\n",
        "\n",
        "This cell finalizes the training setup by addressing hardware compatibility, data streaming efficiency, and optimization strategies. It converts the raw Python generator into a high-performance **TensorFlow Dataset** object.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Optimized Data Generator (final_data_generator)\n",
        "\n",
        "This refined version of the generator includes critical fixes for stability:\n",
        "\n",
        "* **CuDNN Compatibility**: padding='post' is used in pad_sequences. This ensures that zeros are added at the end of the sequence rather than the beginning, which is a requirement for certain GPU-accelerated LSTM kernels (CuDNN).\n",
        "* **Robust Parsing**: Includes logic to handle both tab-separated and comma-separated caption files, preventing runtime errors during data ingestion.\n",
        "* **Feature Validation**: Added a check (if image_id not in image_features) to skip missing data points, ensuring the training loop doesn't crash if an image failed feature extraction.\n",
        "\n",
        "## 2. TensorFlow Dataset Integration\n",
        "\n",
        "To maximize GPU utilization, the generator is wrapped in tf.data.Dataset:\n",
        "\n",
        "* **Output Signature**: Defines the exact TensorSpec (shapes and data types) for the image vectors, caption sequences, and target words. This allows TensorFlow to pre-allocate memory and optimize data flow.\n",
        "* **Lazy Loading**: Data is generated only when needed, keeping the RAM footprint low.\n",
        "\n",
        "## 3. Training Callbacks and Optimization\n",
        "\n",
        "The training process is governed by two key automated behaviors:\n",
        "\n",
        "* **Early Stopping**: Monitors val_loss and stops training if the model fails to improve for 3 consecutive epochs (patience=3). It also restores the best performing weights.\n",
        "* **Learning Rate Decay**: The lr_scheduler reduces the learning rate exponentially as epochs progress. This allows the model to make large updates early on and smaller, more precise adjustments as it converges.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Execution Parameters\n",
        "\n",
        "The model.fit call kicks off the actual learning process:\n",
        "\n",
        "* **Epochs**: Set to 15, providing enough time for the LSTM to learn language patterns.\n",
        "* **Steps per Epoch**: Calculated as Total Captions / Batch Size, ensuring the model sees every sample exactly once per epoch.\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "| --- | --- | --- |\n",
        "| **Padding** | post | GPU/CuDNN Hardware Acceleration. |\n",
        "| **Max Epochs** | 15 | Total training iterations. |\n",
        "| **Learning Rate** | Dynamic | Prevents overshooting the global minimum. |\n",
        "| **Validation** | Enabled | Real-time monitoring of generalization. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_CNO8-Yx0ku7",
        "outputId": "2284e449-f095-45c4-b897-b32346a001d1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "\n",
        "# --- 1. Redefine Generator with 'padding=post' (Fixes GPU/CuDNN Error) ---\n",
        "def final_data_generator(captions, image_features, tokenizer, max_len, batch_size, vocab_size):\n",
        "    num_samples = len(captions)\n",
        "    while True:\n",
        "        # Shuffle to help training\n",
        "        np.random.shuffle(captions)\n",
        "\n",
        "        for start_idx in range(0, num_samples, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, num_samples)\n",
        "            X_images, X_captions, y = [], [], []\n",
        "\n",
        "            for caption_line in captions[start_idx:end_idx]:\n",
        "                # Handle both tab (original) and comma (your file) splits\n",
        "                if '\\t' in caption_line:\n",
        "                    image_id, caption_text = caption_line.strip().split('\\t', 1)\n",
        "                else:\n",
        "                    parts = caption_line.strip().split(',')\n",
        "                    image_id = parts[0]\n",
        "                    caption_text = \" \".join(parts[1:])\n",
        "\n",
        "                if image_id not in image_features:\n",
        "                    continue\n",
        "\n",
        "                # Tokenize\n",
        "                seq = tokenizer.texts_to_sequences([caption_text])[0]\n",
        "\n",
        "                for i in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "\n",
        "                    # CRITICAL FIX: padding='post' puts zeros at the end.\n",
        "                    # This satisfies the GPU/CuDNN requirement.\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_len, padding='post')[0]\n",
        "\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "                    X_images.append(image_features[image_id])\n",
        "                    X_captions.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            # Yield as TUPLE (Image, Caption), Target\n",
        "            if len(X_images) > 0:\n",
        "                yield (np.array(X_images), np.array(X_captions)), np.array(y)\n",
        "\n",
        "# --- 2. Define Output Signature (Fixes TypeError) ---\n",
        "output_signature = (\n",
        "    (\n",
        "        tf.TensorSpec(shape=(None, 2048), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, max_caption_length), dtype=tf.int32)\n",
        "    ),\n",
        "    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "# --- 3. Create Datasets ---\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: final_data_generator(train_captions, train_image_features, tokenizer, max_caption_length, batch_size_train, vocab_size),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: final_data_generator(val_captions, val_image_features, tokenizer, max_caption_length, batch_size_val, vocab_size),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "# --- 4. Callbacks (Fixes Float ValueError) ---\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    return float(lr * tf.math.exp(-0.6))\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# --- 5. Train ---\n",
        "print(\"Starting training with corrected padding...\")\n",
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=len(train_captions) // batch_size_train,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=len(val_captions) // batch_size_val,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, lr_schedule]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Performance Visualization\n",
        "\n",
        "This cell generates a line graph to evaluate the model's learning progress over time. By comparing training loss against validation loss, we can diagnose the quality of the fit and determine if the model is learning effectively or simply memorizing the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Plotting Training Dynamics\n",
        "\n",
        "The code uses matplotlib and seaborn to visualize the history object returned by the model.fit() function:\n",
        "\n",
        "* **Training Loss (Red):** Represents how well the model is minimizing error on the data it sees every epoch.\n",
        "* **Validation Loss (Dark Maroon):** Represents the error on the unseen validation set. This is the true indicator of the model's ability to generalize to new images.\n",
        "\n",
        "## 2. Interpreting the Results\n",
        "\n",
        "The shape of these curves provides critical insights into the training health:\n",
        "\n",
        "* **Convergence:** If both lines trend downward and eventually flatten, the model has successfully converged.\n",
        "* **Overfitting:** If the Training Loss continues to drop while the Validation Loss begins to rise, the model is overfitting. The EarlyStopping callback used in the previous step is designed to catch this and stop training at the optimal point.\n",
        "* **Stability:** The markers (o and h) help identify if the loss is decreasing smoothly or fluctuating wildly, which might suggest the learning rate is too high.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Visualization Features\n",
        "\n",
        "* **whitegrid Style:** Provides a clean background with subtle grid lines, making it easier to track the numerical value of the loss across epochs.\n",
        "* **DPI=200:** Ensures a high-resolution export, suitable for technical reports or presentations.\n",
        "* **Epoch Offset:** The code uses x+1 for the x-axis to ensure the plot starts at \"Epoch 1\" instead of the default 0-indexed \"Epoch 0.\"\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Expected Outcome\n",
        "\n",
        "In a successful Image Captioning run, you should see a sharp initial drop in loss as the model learns basic language structure, followed by a slower decline as it begins to associate specific visual features (from the InceptionV3 vectors) with complex descriptive words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2xMKaIi0kxM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7), dpi=200)\n",
        "sns.set_style('whitegrid')\n",
        "plt.plot([x+1 for x in range(len(history.history['loss']))], history.history['loss'], color='#E74C3C', marker='o')\n",
        "plt.plot([x+1 for x in range(len(history.history['loss']))], history.history['val_loss'], color='#641E16', marker='h')\n",
        "plt.title('Train VS Validation', fontsize=15, fontweight='bold')\n",
        "plt.xticks(fontweight='bold')\n",
        "plt.yticks(fontweight='bold')\n",
        "plt.xlabel('Epoch', fontweight='bold')\n",
        "plt.ylabel('Loss', fontweight='bold')\n",
        "plt.legend(['Train Loss', 'Validation Loss'], loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference: Greedy Search Caption Generation\n",
        "\n",
        "This cell defines the greedy_generator function, which is the core logic used to generate text for new, unseen images. Unlike training, where we provide the correct words, this function requires the model to predict one word at a time based on its own previous choices.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Greedy Search Strategy\n",
        "\n",
        "The function follows a **Greedy Search** approach, which is the simplest form of sequence generation:\n",
        "\n",
        "* **Initial Seed**: The process starts with the single word 'start'.\n",
        "* **Iterative Prediction**: In each loop iteration, the model takes the current sequence and the image features to predict the single most likely next word.\n",
        "* **Selection**: np.argmax(prediction) selects the index of the word with the highest probability score.\n",
        "* **Concatenation**: The chosen word is appended to the string, and this new, longer string is fed back into the model for the next step.\n",
        "\n",
        "## 2. Loop Control and Termination\n",
        "\n",
        "The generation process continues until one of two conditions is met:\n",
        "\n",
        "* **Token Match**: If the model predicts the word 'end', the loop breaks, signaling that the sentence is complete.\n",
        "* **Maximum Length**: If the 'end' token is never reached, the loop terminates after max_caption_length steps to prevent infinite recursion or nonsensical run-on sentences.\n",
        "\n",
        "## 3. Input Reshaping and Preprocessing\n",
        "\n",
        "To match the model's expected input shape:\n",
        "\n",
        "* **Sequence Handling**: The text is converted to integers and padded with padding='post' to match the exact training configuration.\n",
        "* **Reshaping**: Both the image features and the text sequence are reshaped into batches of size 1 ((1, cnn_output_dim) and (1, max_caption_length)) because we are processing one image at a time.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Final Formatting\n",
        "\n",
        "Once the loop finishes, the function performs a \"cleanup\" step:\n",
        "\n",
        "* **Token Stripping**: The internal markers start and end are removed from the string.\n",
        "* **Result**: It returns a clean, human-readable sentence (e.g., \"a dog runs through the grass\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7OCLAJx0kzY"
      },
      "outputs": [],
      "source": [
        "def greedy_generator(image_features):\n",
        "    in_text = 'start'\n",
        "    for _ in range(max_caption_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\n",
        "        sequence = pad_sequences([sequence], maxlen=max_caption_length, padding='post').reshape((1,max_caption_length))\n",
        "\n",
        "        prediction = caption_model.predict([image_features.reshape(1,cnn_output_dim), sequence], verbose=0)\n",
        "        idx = np.argmax(prediction)\n",
        "        word = tokenizer.index_word[idx]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "\n",
        "    in_text = in_text.replace('start ', '')\n",
        "    in_text = in_text.replace(' end', '')\n",
        "    return in_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Inference: Beam Search Generator\n",
        "\n",
        "This cell implements **Beam Search**, a sophisticated heuristic search algorithm that improves caption quality by exploring multiple potential word sequences simultaneously. Unlike Greedy Search, which only picks the single best word at each step, Beam Search reduces the risk of choosing a word that seems good locally but leads to a poor sentence overall.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. How Beam Search Works\n",
        "\n",
        "Beam Search expands the model's \"vision\" by maintaining the **Top-K** most likely partial sentences at every step:\n",
        "\n",
        "- **Beam Width ():** The number of parallel sequences tracked (set by K_beams).\n",
        "- **Candidate Expansion:** For every active sequence, the model predicts the next word and calculates the probability for all possibilities.\n",
        "- **Pruning:** After generating all possible continuations, the algorithm sorts them by their cumulative probability and keeps only the top candidates.\n",
        "\n",
        "## 2. Probability Accumulation\n",
        "\n",
        "The function tracks the \"score\" of each sequence using two methods:\n",
        "\n",
        "- **Linear Probability:** Directly adding the Softmax probabilities.\n",
        "- **Log Probability (log=True):** Adding the logarithms of probabilities. This is standard in deep learning to prevent **numerical underflow**, as multiplying many small decimal probabilities can result in numbers too small for a computer to handle accurately.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Algorithm Refinement\n",
        "\n",
        "The code follows a specific sequence of operations:\n",
        "\n",
        "1. **Initialize:** Starts with the start token and a score of 0.0.\n",
        "2. **Predict:** Uses np.argsort(preds[0])[-K_beams:] to find the most likely next word indices.\n",
        "3. **Sort and Slice:** start_word[-K_beams:] ensures that only the best paths survive to the next iteration.\n",
        "4. **Final Selection:** Once the maximum length is reached or the \"beam\" stabilizes, the path with the highest cumulative score is chosen as the final caption.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Post-Processing\n",
        "\n",
        "- **Index-to-Word Conversion:** Translates the winning integer sequence back into human-readable strings using the tokenizer.\n",
        "- **Clean Termination:** Iterates through the list and stops at the 'end' token, ensuring the final string doesn't include the internal markers or unnecessary padding.\n",
        "\n",
        "### Comparison: Greedy vs. Beam Search\n",
        "\n",
        "| Feature         | Greedy Search        | Beam Search                          |\n",
        "| --------------- | -------------------- | ------------------------------------ |\n",
        "| **Complexity**  | Low (Fast)           | Higher (Slower)                      |\n",
        "| **Exploration** | Single Path          | Multiple Paths ()                    |\n",
        "| **Optimal?**    | Rarely               | More likely to find better sentences |\n",
        "| **Risk**        | High (Short-sighted) | Low (Context-aware)                  |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY8_BCeO0k1m"
      },
      "outputs": [],
      "source": [
        "def beam_search_generator(image_features, K_beams=3, log=False):\n",
        "    start = [tokenizer.word_index['start']]\n",
        "    start_word = [[start, 0.0]]\n",
        "\n",
        "    for _ in range(max_caption_length):\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            sequence = pad_sequences([s[0]], maxlen=max_caption_length, padding='post').reshape((1,max_caption_length))\n",
        "\n",
        "            preds = caption_model.predict([image_features.reshape(1,cnn_output_dim), sequence], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-K_beams:]\n",
        "\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                if log:\n",
        "                    prob += np.log(preds[0][w])\n",
        "                else:\n",
        "                    prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "\n",
        "        start_word = temp\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        start_word = start_word[-K_beams:]\n",
        "\n",
        "    start_word = start_word[-1][0]\n",
        "    captions_ = [tokenizer.index_word[i] for i in start_word]\n",
        "\n",
        "    final_caption = []\n",
        "    for i in captions_:\n",
        "        if i != 'end':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation: BLEU Score Analysis\n",
        "\n",
        "This cell defines the quantitative evaluation logic for the model. It uses the **BLEU (Bilingual Evaluation Understudy)** score, which is the standard metric in Natural Language Processing for comparing a machine-generated sentence against one or more human-written reference sentences.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. BLEU Score Fundamentals\n",
        "\n",
        "The function measures the \"precision\" of the model by checking how many words (unigrams) and sequences of words (n-grams) in the predicted caption also appear in the human references:\n",
        "\n",
        "* **BLEU-1 (Unigrams):** Focuses on individual word accuracy (vocabulary coverage).\n",
        "* **BLEU-2/4 (N-grams):** Focuses on word order and fluency by checking sequences of 2 or more words.\n",
        "\n",
        "## 2. Smoothing Function\n",
        "\n",
        "The code implements SmoothingFunction().method1 (Chen & Cherry). This is a critical adjustment for short sequences like image captions.\n",
        "\n",
        "* **The Problem:** Standard BLEU scores drop to  if even a single n-gram (like a 4-gram) is missing.\n",
        "* **The Solution:** Smoothing provides a tiny positive value to missing n-grams so that the overall score remains representative of the words that *did* match.\n",
        "\n",
        "## 3. Weighted Evaluation\n",
        "\n",
        "The function uses specific weights to balance the importance of different n-gram lengths:\n",
        "\n",
        "* **weights_1 (0.3, 0.3, 0.3, 0):** Evaluates matches up to 3-grams.\n",
        "* **weights_2 (0.25, 0.25, 0.25, 0.25):** Evaluates matches up to 4-grams (standard BLEU-4 configuration).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Comparison Logic\n",
        "\n",
        "The function evaluates both generation strategies simultaneously:\n",
        "\n",
        "1. **Greedy Scoring:** Calculates how accurate the most probable word-by-word path was.\n",
        "2. **Beam Search Scoring:** Calculates if the multi-path exploration resulted in a higher-quality, more \"human-like\" caption.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Output Format\n",
        "\n",
        "The function returns a formatted list of strings for easy readability:\n",
        "\n",
        "* **Quantitative Data:** Rounded BLEU-1 and BLEU-2 scores for both methods.\n",
        "* **Qualitative Data:** The actual generated strings for a side-by-side comparison.\n",
        "\n",
        "| Metric | Higher Score Means... |\n",
        "| --- | --- |\n",
        "| **BLEU-1** | The model is using the correct objects/labels. |\n",
        "| **BLEU-2+** | The model is constructing grammatically correct phrases. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhafcD6g0k30"
      },
      "outputs": [],
      "source": [
        "def BLEU_score(actual, greedy, beam_search):\n",
        "    # 1. Setup: Tokenize & Lowercase\n",
        "    # \"actual\" is a list of reference strings. We split them into words.\n",
        "    references = [s.lower().split() for s in actual]\n",
        "\n",
        "    # \"greedy\" and \"beam_search\" are lists containing one prediction string.\n",
        "    greedy_tokens = greedy[0].lower().split()\n",
        "    beam_tokens = beam_search[0].lower().split()\n",
        "\n",
        "    # 2. Define Smoothing Function\n",
        "    # This prevents the score from crashing to 0.0 just because a 4-gram is missing.\n",
        "    chen_cherry = SmoothingFunction()\n",
        "\n",
        "    # \"BLEU-1\" in your tutorial uses (0.3, 0.3, 0.3, 0) -> Looks for up to 3-grams\n",
        "    weights_1 = (0.3, 0.3, 0.3, 0)\n",
        "    # \"BLEU-2\" in your tutorial uses (0.25, 0.25, 0.25, 0.25) -> Looks for up to 4-grams\n",
        "    weights_2 = (0.25, 0.25, 0.25, 0.25)\n",
        "\n",
        "    # 4. Calculate Scores with Smoothing\n",
        "    # Greedy\n",
        "    sg_1 = sentence_bleu(references, greedy_tokens, weights=weights_1, smoothing_function=chen_cherry.method1)\n",
        "    sg_2 = sentence_bleu(references, greedy_tokens, weights=weights_2, smoothing_function=chen_cherry.method1)\n",
        "\n",
        "    # Beam Search\n",
        "    sb_1 = sentence_bleu(references, beam_tokens, weights=weights_1, smoothing_function=chen_cherry.method1)\n",
        "    sb_2 = sentence_bleu(references, beam_tokens, weights=weights_2, smoothing_function=chen_cherry.method1)\n",
        "\n",
        "    # 5. Return in the EXACT format you requested\n",
        "    return [\n",
        "        f'BLEU-2 Greedy: {round(sg_2, 5)}',\n",
        "        f'BLEU-1 Greedy: {round(sg_1, 5)}',\n",
        "        f'Greedy: {greedy[0]}',\n",
        "        f'BLEU-2 Beam Search: {round(sb_2, 5)}',\n",
        "        f'BLEU-1 Beam Search: {round(sb_1, 5)}',\n",
        "        f'Beam Search:  {beam_search[0]}'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Prediction on Test Data\n",
        "\n",
        "This cell executes the inference phase for the entire test set. It uses the trained model to generate captions for images it has never seen before, allowing for a final evaluation of the system's performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Automated Caption Generation\n",
        "\n",
        "The loop iterates through the test_image_features dictionary to produce descriptions for each held-out image:\n",
        "\n",
        "* **greedy_generator**: The function predicts the sequence of words word-by-word, choosing the most likely token at each step based on the image features.\n",
        "* **Storage**: Results are saved in the generated_captions dictionary, mapped by image_id. This structured storage makes it easy to compare the AI's output against the ground-truth human captions later.\n",
        "\n",
        "## 2. Progress Monitoring\n",
        "\n",
        "Since generating text for hundreds of images involves thousands of individual model predictions, the tqdm_notebook progress bar is utilized:\n",
        "\n",
        "* **Real-time Feedback**: Tracks the completion percentage and estimated time remaining.\n",
        "* **Visual Cue**: The green progress bar confirms that the inference loop is running smoothly without errors.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Workflow Context\n",
        "\n",
        "This step serves as the bridge between **Model Training** and **Final Evaluation**:\n",
        "\n",
        "1. **Extraction**: (Completed) Image features were extracted using InceptionV3.\n",
        "2. **Training**: (Completed) The LSTM learned to map features to words.\n",
        "3. **Inference**: (**This Step**) The model \"imagines\" captions for new visual data.\n",
        "4. **Scoring**: (Next Step) The generated text will be compared to human references using BLEU scores.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Key Metrics to Watch\n",
        "\n",
        "After this loop finishes, the generated_captions dictionary will contain the raw material needed to calculate:\n",
        "\n",
        "* **Linguistic Diversity**: How many unique words did the model use across the test set?\n",
        "* **Average Length**: Are the generated captions consistently shorter or longer than the training data?\n",
        "* **Semantic Accuracy**: Do the words actually match the objects present in the images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9465Qfrb0k8D"
      },
      "outputs": [],
      "source": [
        "generated_captions = {} \n",
        "\n",
        "pbar = tqdm_notebook(total=len(test_image_features), position=0, leave=True, colour='green')\n",
        "for image_id in test_image_features:\n",
        "    cap = greedy_generator(test_image_features[image_id])\n",
        "    generated_captions[image_id] = cap\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Alignment: Ground Truth Preparation\n",
        "\n",
        "This cell creates a dedicated mapping of original, human-written descriptions for the test dataset. This \"Ground Truth\" is essential for the final evaluation phase, where we compare the model's AI-generated text against the actual descriptions provided by humans.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Ground Truth Dictionary\n",
        "\n",
        "The code constructs the test_actual_captions dictionary using a logic similar to the training data preparation, but specifically filtered for the **Test Set**:\n",
        "\n",
        "* **Key**: The unique Image ID (e.g., 12345.jpg).\n",
        "* **Value**: A list containing all five human-provided captions for that specific image.\n",
        "* **Format Flexibility**: The code includes a conditional check to handle both tab-separated (\\t) and comma-separated (,) file formats, ensuring the script is robust across different versions of the Flickr8K dataset.\n",
        "\n",
        "## 2. Normalization for Evaluation\n",
        "\n",
        "* **caption_text.lower()**: This is a critical step for scoring. Since the model was trained on cleaned, lowercase text, the reference captions must also be in lowercase.\n",
        "* **Case Sensitivity**: Without this normalization, a match between \"Dog\" (Reference) and \"dog\" (Prediction) would be ignored by the BLEU score algorithm, leading to artificially low performance metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. The Comparison Framework\n",
        "\n",
        "By the end of this cell, we have two perfectly aligned structures:\n",
        "\n",
        "1. **generated_captions**: What the model *thinks* is in the image (1 caption per image).\n",
        "2. **test_actual_captions**: What humans *know* is in the image (5 captions per image).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why 5 Captions per Image?\n",
        "\n",
        "In Image Captioning, there is no single \"correct\" answer. One person might describe an image as \"a dog running,\" while another says \"a golden retriever in the grass.\" By comparing our model's output against **five** different references, we get a much fairer and more accurate BLEU score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egDjQYVccYdE"
      },
      "outputs": [],
      "source": [
        "# Create the missing dictionary 'test_actual_captions'\n",
        "test_actual_captions = {}\n",
        "\n",
        "# Iterate over your original 'captions' list\n",
        "for line in captions:\n",
        "    line = line.strip()\n",
        "    # Handle CSV split\n",
        "    if '\\t' in line:\n",
        "        image_id, caption_text = line.split('\\t', 1)\n",
        "    else:\n",
        "        parts = line.split(',')\n",
        "        image_id = parts[0]\n",
        "        caption_text = \" \".join(parts[1:])\n",
        "\n",
        "    if image_id in test_image_ids:\n",
        "        if image_id not in test_actual_captions:\n",
        "            test_actual_captions[image_id] = []\n",
        "        test_actual_captions[image_id].append(caption_text.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qualitative Results: Image, Prediction, and BLEU Analysis\n",
        "\n",
        "This final visualization function brings all components together‚Äîthe raw visual data, the model's linguistic interpretation, and the mathematical validation‚Äîinto a single scannable interface. It provides a \"gut check\" on how the model actually performs in the real world.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Visual Comparison Logic\n",
        "\n",
        "The function visualization performs a comprehensive assessment of 7 randomly selected images from the test set:\n",
        "\n",
        "* **Ground Truth Cleanup**: It strips the internal start and end tokens from the reference captions to make them human-readable.\n",
        "* **Dual Inference**: It runs both the **Greedy** and **Beam Search** generators for the same image to see which one produces a more natural description.\n",
        "* **Score Integration**: It calls the BLEU_score evaluator to instantly display the performance metrics alongside the text.\n",
        "\n",
        "## 2. Layout Structure\n",
        "\n",
        "The output is organized as a vertical gallery:\n",
        "\n",
        "* **Left Column**: Displays the actual test image, resized for the grid.\n",
        "* **Right Column**: A text-based dashboard containing:\n",
        "* The **Greedy Search** prediction.\n",
        "* The **Beam Search** prediction.\n",
        "* **BLEU-1 and BLEU-2 scores** for both methods, providing immediate feedback on how closely the AI matched human consensus.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Interpreting the Outputs\n",
        "\n",
        "When you run this cell, you can look for three specific behaviors:\n",
        "\n",
        "1. **Semantic Alignment**: Does the model identify the correct objects (e.g., \"dog\", \"ball\", \"grass\")?\n",
        "2. **Grammar and Fluency**: Is the sentence structure logical, or is it a \"bag of words\"?\n",
        "3. **Beam vs. Greedy**: Does Beam Search (which looks ahead) provide a more complex or accurate description than the simple Greedy approach?\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Summary of the Pipeline\n",
        "\n",
        "This concludes the end-to-end Image Captioning workflow:\n",
        "\n",
        "1. **Load/Clean**: Prepared the Flickr8K dataset.\n",
        "2. **Encoder**: Extracted visual features with InceptionV3.\n",
        "3. **Decoder**: Built an LSTM network to generate sequences.\n",
        "4. **Training**: Optimized the model with Adam and EarlyStopping.\n",
        "5. **Evaluation**: Quantified performance with BLEU and visualized results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mate0uD0k9v"
      },
      "outputs": [],
      "source": [
        "def visualization(data, greedy_caps, beamS_generator, evaluator, num_of_images): #20\n",
        "    keys = list(data.keys())\n",
        "    images = [np.random.choice(keys) for i in range(num_of_images)] # Randomly selected images\n",
        "    count = 1\n",
        "    fig = plt.figure(figsize=(6,20))\n",
        "    for filename in images:\n",
        "        actual_cap = data[filename]\n",
        "        actual_cap = [x.replace(\"start \", \"\") for x in actual_cap]\n",
        "        actual_cap = [x.replace(\" end\", \"\") for x in actual_cap]\n",
        "\n",
        "        greedy_cap = greedy_caps[filename]\n",
        "        beamS_cap = beamS_generator(test_image_features[filename])\n",
        "\n",
        "        caps_with_score = evaluator(actual_cap, [greedy_cap]*(len(actual_cap)), [beamS_cap]*(len(actual_cap)))\n",
        "\n",
        "        image_load = load_img(images_directory+filename, target_size=(199,199,3))\n",
        "        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n",
        "        ax.imshow(image_load)\n",
        "        count += 1\n",
        "\n",
        "        ax = fig.add_subplot(num_of_images,2,count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,len(caps_with_score))\n",
        "        for i, text in enumerate(caps_with_score):\n",
        "            ax.text(0,i,text,fontsize=10)\n",
        "        count += 1\n",
        "    plt.show()\n",
        "\n",
        "visualization(test_actual_captions, generated_captions, beam_search_generator, BLEU_score, 7)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09d9b5b452bc43cb8f66669966e8d19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33f2e715f0294c1c9cefcabffaf05545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_866e28c31ed741619e5b4c0274241c55",
              "IPY_MODEL_51aea74cf7c84971912bac863971a969",
              "IPY_MODEL_9973401db03a44d285dc1ae5b69f5196"
            ],
            "layout": "IPY_MODEL_9cc5e9fb565743f49eb9f8425da3fdf1"
          }
        },
        "51aea74cf7c84971912bac863971a969": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f5fc864c383485dbb2acb6b191a646c",
            "max": 8091,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cff09414f7714ab3b66d356935467ccf",
            "value": 30
          }
        },
        "68f9dc1e5a704ce3b0011a723e6e0533": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "866e28c31ed741619e5b4c0274241c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d819267408164696b9ccd54b139a920d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09d9b5b452bc43cb8f66669966e8d19b",
            "value": "‚Äá‚Äá0%"
          }
        },
        "8f5fc864c383485dbb2acb6b191a646c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9973401db03a44d285dc1ae5b69f5196": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a125a3403d1b4c28a7f3498942248e10",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_68f9dc1e5a704ce3b0011a723e6e0533",
            "value": "‚Äá30/8091‚Äá[00:17&lt;1:24:50,‚Äá‚Äá1.58it/s]"
          }
        },
        "9cc5e9fb565743f49eb9f8425da3fdf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a125a3403d1b4c28a7f3498942248e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff09414f7714ab3b66d356935467ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "d819267408164696b9ccd54b139a920d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
